{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from twython import Twython\n",
    "\n",
    "# Imports the keys from the python file\n",
    "from twitter_key import api_key, api_secret\n",
    "\n",
    "# Import some additional libraries that will allow us to plot and interact with the operating system\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assigns the keys to the variables\n",
    "APP_KEY = api_key\n",
    "APP_SECRET = api_secret\n",
    "\n",
    "# Create a Twython object called Twitter\n",
    "# Set this up using your Twitter Keys\n",
    "# Say we are going to use OAuth 2\n",
    "twython_setup = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
    "\n",
    "# Get an OAuth2 access token, save as variable so we can launch our \n",
    "OAUTH2_ACCESS_TOKEN = twython_setup.obtain_access_token()\n",
    "\n",
    "# Create a Twython Object we will use for our access to the API\n",
    "my_twython = Twython(APP_KEY, access_token=OAUTH2_ACCESS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions for Twitter scraper defined here\n",
    "\n",
    "def continuous_scrape(duration):\n",
    "    \"\"\"Gets maximum allowed number of tweets from Twitter API multiple times.\n",
    "    Operates in 15 minute cycles.\n",
    "    Saves results as json.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of times the code will be executed. \n",
    "    print (\"Initiating continuous scraping for \" + str(duration) + \" minutes.\")\n",
    "    cycles = float(duration/15)\n",
    "    while cycles > 0 and duration > 0:\n",
    "        # Uses try/except structure to handle API errors.\n",
    "        try:\n",
    "            t = get_max_tweets(duration, latlong)\n",
    "            # Names file by time.\n",
    "            timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            # We write a new JSON into the target path\n",
    "            print (\"Writing file \" + \"%stweets.json\" %(timestr + \"_\" + search_term + \"_\"))\n",
    "            with open( 'data/' + '%stweets.json' %(timestr + \"_\" + search_term), 'w' ) as f:\n",
    "                f.write(json.dumps(t))\n",
    "            cycles -= 1\n",
    "            duration -= 15\n",
    "        except:\n",
    "            pass\n",
    "    print (\"All done.\")\n",
    "\n",
    "    \n",
    "def get_max_tweets(duration, latlong=0):\n",
    "    \"\"\"Gets tweets at maximum allowable rate of 450 tweets/15 mins.\n",
    "    Rate is fixed in function, but frequency of request can be adjusted.\n",
    "    Will run for up to 15 minutes.\n",
    "    Returns dictionary of results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary to parse the JSON\n",
    "    all_tweets = {}\n",
    "    \n",
    "    # Set length of time function will run, with a max time of 15 minutes\n",
    "    if duration > 15:\n",
    "        total_time = 900\n",
    "    elif duration <= 15:\n",
    "        total_time = duration*60\n",
    "    print (\"Remaining time = \" + str(duration) + \" minutes.\")    \n",
    "    \n",
    "    # Adjust the number_of_tweets to determine frequency of API requests\n",
    "    number_of_tweets = 30\n",
    "    \n",
    "    remaining_seconds = total_time\n",
    "    \n",
    "    # Waiting interval is set to produce API request rate of 450 tweets/15 mins\n",
    "    interval = number_of_tweets*2\n",
    "    while remaining_seconds > 0: \n",
    "        added = 0\n",
    "        # Hit the Twitter API using our function\n",
    "        new_tweets = get_tweets_by_location(number_of_tweets, latlong)\n",
    "        # Parse the resulting JSON, and save the rest of the raw content\n",
    "        for tweet in new_tweets:\n",
    "            tid = tweet['id']\n",
    "            if tid not in all_tweets:\n",
    "                properties = {}\n",
    "                if tweet['coordinates'] != None:\n",
    "                    properties['lat'] = tweet['coordinates']['coordinates'][0]\n",
    "                    properties['lon'] = tweet['coordinates']['coordinates'][1]\n",
    "                else:\n",
    "                    properties['lat'] = None\n",
    "                    properties['lon'] = None\n",
    "                properties['location'] = tweet['user']['location'] #This will get us the location associated with the profile\n",
    "                properties['tweet_id'] = tid\n",
    "                properties['content'] = tweet['text']\n",
    "                properties['user'] = tweet['user']['id']\n",
    "                properties['raw_source'] = tweet\n",
    "                properties['data_point'] = 'none'\n",
    "                properties['time'] = tweet['created_at']\n",
    "                properties['language'] = tweet['lang']\n",
    "                all_tweets[ tid ] = properties\n",
    "                added += 1\n",
    "        print(\"At %d seconds, added %d new tweets, for a total of %d\" % ( total_time - remaining_seconds, added, len( all_tweets )))\n",
    "        time.sleep(interval)\n",
    "        remaining_seconds -= interval\n",
    "    print(str(len(all_tweets)) + ' Tweets retrieved.')\n",
    "    # We return the final dictionary to work with in Python\n",
    "    return all_tweets\n",
    "\n",
    "\n",
    "def get_tweets_by_location(number_of_tweets, latlong=None):\n",
    "    \"\"\"Gets specified number of tweets with a given query at a given lat-long.\n",
    "    Returns results as a list.\n",
    "    \"\"\" \n",
    "    \n",
    "    # Uses the search function to hit the APIs endpoints and look for recent tweets within the area\n",
    "    results = my_twython.search(q=search_term, geocode=str(latlong[0])+','+str(latlong[1])+','+ distance, result_type=type_of_result, count=number_of_tweets)\n",
    "    # Returns the only the statuses from the resulting JSON\n",
    "    return results['statuses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize search variables and scraper duration here\n",
    "\n",
    "# Sets search term, can be blank for all tweets\n",
    "search_term='FBI' \n",
    "\n",
    "# Sets a Lat Lon\n",
    "latlong=[42.3601,-71.0942] # Set to MIT campus, Cambridge, MA, USA\n",
    "\n",
    "# Sets search radius\n",
    "distance='20mi'\n",
    "\n",
    "# Sets result type: 'recent', 'popular', or 'mixed'\n",
    "type_of_result='mixed'\n",
    "\n",
    "# Sets length of time in minutes that Twitter scraper will run\n",
    "duration = 30\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating continuous scraping for 30 minutes.\n",
      "Remaining time = 30 minutes.\n",
      "At 0 seconds, added 1 new tweets, for a total of 1\n",
      "At 60 seconds, added 0 new tweets, for a total of 1\n",
      "At 120 seconds, added 1 new tweets, for a total of 2\n",
      "At 180 seconds, added 0 new tweets, for a total of 2\n",
      "At 240 seconds, added 0 new tweets, for a total of 2\n",
      "At 300 seconds, added 0 new tweets, for a total of 2\n",
      "At 360 seconds, added 0 new tweets, for a total of 2\n",
      "At 420 seconds, added 0 new tweets, for a total of 2\n",
      "At 480 seconds, added 0 new tweets, for a total of 2\n",
      "At 540 seconds, added 0 new tweets, for a total of 2\n",
      "At 600 seconds, added 0 new tweets, for a total of 2\n",
      "At 660 seconds, added 0 new tweets, for a total of 2\n",
      "At 720 seconds, added 0 new tweets, for a total of 2\n",
      "At 780 seconds, added 0 new tweets, for a total of 2\n",
      "At 840 seconds, added 0 new tweets, for a total of 2\n",
      "2 Tweets retrieved.\n",
      "Writing file 20170306-101304_FBI_tweets.json\n",
      "Remaining time = 15 minutes.\n",
      "At 0 seconds, added 2 new tweets, for a total of 2\n",
      "At 60 seconds, added 0 new tweets, for a total of 2\n",
      "At 120 seconds, added 0 new tweets, for a total of 2\n",
      "At 180 seconds, added 0 new tweets, for a total of 2\n",
      "At 240 seconds, added 0 new tweets, for a total of 2\n",
      "At 300 seconds, added 0 new tweets, for a total of 2\n",
      "At 360 seconds, added 0 new tweets, for a total of 2\n",
      "At 420 seconds, added 0 new tweets, for a total of 2\n",
      "At 480 seconds, added 0 new tweets, for a total of 2\n",
      "At 540 seconds, added 0 new tweets, for a total of 2\n",
      "At 600 seconds, added 0 new tweets, for a total of 2\n",
      "At 660 seconds, added 0 new tweets, for a total of 2\n",
      "At 720 seconds, added 0 new tweets, for a total of 2\n",
      "At 780 seconds, added 0 new tweets, for a total of 2\n",
      "At 840 seconds, added 0 new tweets, for a total of 2\n",
      "2 Tweets retrieved.\n",
      "Writing file 20170306-102806_FBI_tweets.json\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# Runs Twitter scraper\n",
    "\n",
    "continuous_scrape (duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def json_to_df (folder):\n",
    "    \"\"\"Reads json files from given directory into new pandas dataframe.\n",
    "    Takes name of folder where files are saved.\n",
    "    Returns dataframe.\"\"\"\n",
    "\n",
    "    # specifies location of files\n",
    "    file_dir = folder \n",
    "    # gets only the files we have saved\n",
    "    onlyfiles = [ f for f in listdir(file_dir) if isfile(join(file_dir,f)) and not f.startswith('.')]\n",
    "\n",
    "    # creates empty dataframe with columns for each property\n",
    "    df = pd.DataFrame(columns = ['tweet_id', 'lat', 'lon', 'content','language','location','user','raw_source','data_point','time'])\n",
    "\n",
    "    # Loops through all json files to create single, compiled file\n",
    "    for file in onlyfiles:\n",
    "        full_dir = join(file_dir,file) \n",
    "        with open(full_dir) as json_data:\n",
    "            dict = json.load(json_data) \n",
    "            if not isinstance(dict, list):\n",
    "                for key, val in dict.items():\n",
    "                    df.loc[key,val] = val\n",
    "    return df\n",
    "   \n",
    "    \n",
    "def count_attribute (df, attribute):\n",
    "    \"\"\"Creates data frame counting any one variable of a dataframe.\n",
    "    Takes name of dataframe, attribute name.\n",
    "    Returns new dataframe of counts.\"\"\"\n",
    "\n",
    "    grouped = df.groupby(attribute)\n",
    "    count = grouped[attribute].count()\n",
    "    df_count = count.to_frame()\n",
    "    df_count.columns = ['Count']\n",
    "    df_count.index.names = [attribute]\n",
    "    df_count.sort_index()\n",
    "    return df_count\n",
    "\n",
    "    \n",
    "def make_piechart (df):\n",
    "    \"\"\"Makes a pie chart of dataframe data.\n",
    "    Takes dataframe with column 'Count'.\"\"\"\n",
    "    \n",
    "    # Create a list of colors (from iWantHue)\n",
    "    colors = [\"#697dc6\",\"#5faf4c\",\"#7969de\",\"#b5b246\",\n",
    "              \"#cc54bc\",\"#4bad89\",\"#d84577\",\"#4eacd7\",\n",
    "              \"#cf4e33\",\"#894ea8\",\"#cf8c42\",\"#d58cc9\",\n",
    "              \"#737632\",\"#9f4b75\",\"#c36960\"]\n",
    "\n",
    "    # Create a pie chart\n",
    "    plt.pie( df['Count'], labels=df.index.get_values(), shadow=False, colors=colors)\n",
    "\n",
    "    # View the plot drop above\n",
    "    plt.axis('equal')\n",
    "\n",
    "    # View the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def make_scatterplot (df):\n",
    "    \n",
    "    # Create a filter from df_tweets filtering only those that have values for lat and lon\n",
    "    df_tweets_with_location = df_tweets[df_tweets.lon.notnull() & df_tweets.lat.notnull()]\n",
    "    df_tweets_with_location\n",
    "    \n",
    "    # Use a scatter plot to make a quick visualization of the data points\n",
    "    # NOTE: WHEN I DID THIS, I ONLY HAD SIX OUT OF ABOUT 100 TWEETS!\n",
    "    plt.scatter(df_tweets_with_location['lon'],df_tweets_with_location['lat'], s=25)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize variables for data analysis \n",
    "folder = \"data\"\n",
    "attribute = 'location'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform data analysis\n",
    "\n",
    "df_tweets = json_to_df (folder)\n",
    "#automagically clean data - remove duplicates\n",
    "df_attribute_count = count_attribute (df_tweets, attribute)\n",
    "\n",
    "#df_tweets_clean = manually clean data\n",
    "\n",
    "make_piechart (df_attribute_count)\n",
    "make_scatterplot\n",
    "\n",
    "#export df_tweets_clean data to CSV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
